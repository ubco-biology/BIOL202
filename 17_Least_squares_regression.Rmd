# Least-squares linear regression {#ls_regression}

```{r echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = "html")
```

**Tutorial learning objectives**

* Learn about using least-squares linear regression ("LSR", also called "Model-1 regression") to model the relationship between two numeric variables, and to make predictions  
* Learn the assumptions of LSR, and how to test whether these assumptions are met
* Learn that the implied statistical null hypothesis for a LSR is that the slope of the relationship is equal to zero
* Learn how to interpret regression analysis output
* Learn about the "coefficient of determination", $R^2$, represents the fraction of variation in the response variable that is accounted for by the regression model 
* Learn how to make predictions using LRS
* Learn how to implement and report the findings of LSR  

## Load packages and import data {#LSR_packages_data}

Load the `tidyverse`, `skimr`, `broom`, and `knitr`.

```{r lsr_packages, warning = FALSE, message = FALSE}
library(tidyverse)
library(skimr)
library(broom)
library(knitr)
```

We'll use the "plantbiomass" dataset:  

```{r import_data_lsr}
plantbiomass <- read_csv("https://raw.githubusercontent.com/ubco-biology/BIOL202/main/data/plantbiomass.csv")
```

The `plantbiomass` dataset (Example 17.3 in the text book) includes data describing plant biomass measured after 10 years within each of 5 experimental "species richness" treatments, wherein plants were grown in groups of 1, 2, 4, 8, or 16 species. The treatment variable is called `nSpecies`, and the response variable `biomassStability` is a measure of ecosystem stability. The research hypothesis was that increasing diversity (species richness) would lead to increased ecosystem stability.  

Below is a visualization of the data.

```{r lsrfigzero, echo = FALSE, fig.cap = "Stability of biomass production over 10 years in 161 plots and the initial number of plant species assigned to plots.", fig.width = 4.2, fig.height = 4} 

plantbiomass %>%
ggplot(aes(x = nSpecies, y = biomassStability)) +
  geom_point(shape = 1) +
  xlab("Species richness") +
  ylab("Biomass stability") +
  theme_bw()
```

<div class="note">
When one [visualizes the data](#LSR_vis), one might ask why an [ANOVA](#do_anova1) isn't the analysis method of choice. If we were simply interested in testing the null hypothesis that "there is no difference in mean ecosystem stability among the species richness treatment groups", then an ANOVA could be used, and we would simply treat the "species richness" variable as a categorical variable. However, here we are interested not simply in testing for differences among treatment groups, but more specifically in quantifying if and how ecosystem stability varies with variation in species richness, AND if so, whether we can reliably predict ecosystem stability based on species richness. For this, we need to construct a regression model.
</div>

Let's have a look at the data:

```{r view_data_lsr}

plantbiomass %>% skim_without_charts()
```

We see that both variables are numeric, and that there are 161 observations overall.  

* * *

## Least-squares regression analysis {#LSR}

When we are only interested in the strength of a linear association between two numerical variables, we use a [correlation analysis](#two_num).  

When we are interest in making _predictions_, we use regression analysis.  

Often in the literature you see authors reporting regression results when in fact there was no rationale for using regression; a correlation analysis would have been more appropriate.  

The two analyses are mathematically related.  

Regression analysis is extremely common in biology, and unfortunately it is also common to see incorrect implementation of regression analysis. In particular, one often sees scatterplots that clearly reveal violations of the assumptions of regression analysis (see below).  

>Failure to appropriately check assumptions can lead to misleading and incorrect conclusions   

* * *

### Equation of a line and "least-squares line" {#LSR_line}

Recall from your high-school training that the equation for a straight line is:  

$Y$ = _a_ + _bX_  

In regression analysis, the "least-squares line" is the line for which the sum of all the _squared deviations in Y_ is smallest.  

In a regression context, _a_ is the Y-intercept and _b_ is the slope of the regression line.  

Regression analysis falls under the heading of **inferential statistics**, which means we use it to draw inferences about a linear relationship between $Y$ and $X$ within a _population_ of interest. So in the actual population, the true least-squares line is represented as follows:  

$Y$ = $\alpha$ + $\beta$$X$  

In practice, we estimate the "parameters" $\alpha$ and $\beta$ using a random sample from the population, and by calculating _a_ and _b_ using regression analysis.  

<div class="flag">
The $\alpha$ in the regression equation has no relation to the $\alpha$ that sets the significance level for a test!
</div>

<div class="note">
The equation for the true least-squares line shown above uses the equation shown in the course text book.  However, a more complete and conventional way to write the equation is as follows:  

$Y$ = $\beta_0$ + $\beta_1$$x$ + $\epsilon$

</div>
* * *

### Hypothesis testing or prediction? {#LSR_hyp}

In general, there are two common scenarios in which least-squares regression analysis is appropriate.  

* We wish to know whether some numeric variable $Y$ can be reliably predicted using a regression model of $Y$ on some predictor $X$.  For example, "Can vehicle mileage be reliably predicted by the weight of the vehicle?"  

* We wish to test a specific hypothesis about the slope of the regression model of $Y$ on $X$.  For example, across species, basal metabolic rate (BMR) relates to body mass (M) according to $BMR = \alpha M^\beta$.  Here, $\alpha$ is a constant.  If we log-transform both sides of the equation, we get the following relationship, for which the parameters can be estimated via linear regression analysis:   

$log(BMR) = log(\alpha) + \beta\cdot log(M)$ 

Some biological hypotheses propose that the slope $\beta$ should equal 2/3, whereas others propose 3/4.  This represents an example of when one would propose a specific, hypothesized value for $\beta$.  

In the most common application of regression analysis we don't propose a specific value for the slope $\beta$.  Instead, we proceed with the "default" and **unstated** null and alternative hypotheses as follows:  

**H~0~**: $\beta = 0$  
**H~A~**: $\beta \neq 0$     

In this case, **there is no need to explicitly state these hypotheses for regression analysis**.  

You must, however, know what is being tested: the null hypothesis that the slope is equal to zero.  

### Steps to conducting regression analysis {#LSR_steps}

In regression analysis one must implement the analysis before one can check the assumptions. Thus, the order of operations is a bit different from that of other tests.  

**Follow these steps when conducting regression analysis:**

* Set an $\alpha$ level (default is 0.05)  
* Provide an appropriate figure, including figure caption, to visualize the association between the [two numeric variables](#two_numeric) 
* Provide a sentence or two interpreting your figure, and this may inform your concluding statement  
* Conduct the regression analysis, save the output, and use the **residuals** from the analysis to check assumptions  
* Assumptions  
    + state the assumptions of the test  
    + use appropriate figures and / or tests to check whether the assumptions of the statistical test are met
    + transform data to meet assumptions if required
    + if assumptions can't be met (e.g. after transformation), use alternative methods (**these alternative methods are not covered in this course**)
    + if data transformation is required, then do the transformation and provide another appropriate figure, including figure caption, to visualize the transformed data, and a sentence or two interpreting this new figure  
* Calculate the confidence interval for the slope and the $R^2$ value, report these in concluding statement  
* If the slope of the least-squares regression line differs significantly from zero (i.e. the regression is significant), provide a scatterplot that includes the regression line and **confidence bands**, with an appropriate figure caption  
* Use the output from the regression analysis (above) draw an appropriate conclusion and communicate it clearly  

* * *

### State question and set the $\alpha$ level {#LSR_alpha}     

The `plantbiomass` dataset (Example 17.3 in the text book) includes data describing plant biomass measured after 10 years within each of 5 experimental "species richness" treatments, wherein plants were grown in groups of 1, 2, 4, 8, or 16 species. The treatment variable is called `nSpecies`, and the response variable `biomassStability` is a measure of ecosystem stability. The research hypothesis was that increasing diversity (species richness) would lead to increased ecosystem stability. If the data support this hypothesis, it would be valuable to be able to predict ecosystem stability based on species richness.

We will use the conventional $\alpha$ level of 0.05.  

* * *

### Visualize the data {#LSR_vis}     

Using the plant biomass data, let's evaluate whether ecosystem stability can be reliably predicted from plant species richness.  

We learned in an earlier [tutorial](#two_numeric) that the best way to visualize an association between two numeric variables is with a scatterplot, and that we can create a scatterplot using the `ggplot` and `geom_point` functions:  

```{r lsrfigone, fig.cap = "Stability of biomass production over 10 years in 161 plots and the initial number of plant species assigned to plots.", fig.width = 4.2, fig.height = 4} 

plantbiomass %>%
ggplot(aes(x = nSpecies, y = biomassStability)) +
  geom_point(shape = 1) +
  xlab("Species richness") +
  ylab("Biomass stability") +
  theme_bw()
```

Note that in the above figure we do not report units for the response variable, because in this example the variable does not have units.  But often you do need to provide units!

<div class="note">
This figure that you first produce for visualizing the data may or may not be necessary to present with your results. If your least-squares regression does end up being statistically significant, i.e. having a slope that differs significantly from zero, then you'll create a new figure to reference in your concluding statement (see below). If your regression analysis is non-significant, then you should present and refer to this simple scatterplot (which does not include a best-fit regression line).
</div>

* * *

### Interpreting a scatterplot {#LSR_scatter_interp}

In an earlier [tutorial](#interpret_scatter), we learned how to properly interpret a scatterplot, and **what information should to include in your interpretation**. Be sure to consult that tutorial.   

> We see in Figure \@ref(fig:lsrfigone) that there is a weak, positive, and somewhat linear association between biomass stability and Species richness.  There appears to be increasing spread of the response variable with increasing values of the predictor (x) variable, and perhaps outliers in the treatment group with greatest species richness. We should keep this in mind.   

* * *

### Checking assumptions of regression analysis {#LSR_assumptions}

Regression analysis assumes that:  

* the true relationship between $X$ and $Y$ is linear  
* for every value of $X$ the corresponding values of $Y$ are normally distributed  
* the variance of $Y$-values is the same at all values of $X$
* at each value of $X$, the $Y$ measurements represent a random sample from the population of possible $Y$ values  

Strangely, we must implement the regression analysis before checking the assumptions. This is because we check the assumptions using the **residuals** from the regression analysis.  

Let's implement the regression analysis and be sure to store the output for future use.   

The function to use is the `lm` function, which we saw in a previous [tutorial](#do_anova1), but here our predictor (independent) variable is a numeric variable rather than a categorical variable.  

Let's run the regression analysis and assign the output to an object as follows:  

```{r LSR_run}
biomass.lm <- lm(biomassStability ~ nSpecies, data = plantbiomass)
```

Let's have a look at the untidy output:

```{r LSR_view_untidy}
biomass.lm
```

We'll learn a bit later how to tidy up this output...   

Before we do anything else (e.g. explore the results of the regression), we need to first check the assumptions!  

To get the residuals, which are required to check the assumptions, you first need to run the `augment` function from the `broom` package on the untidy regression object, as this will allow us to easily access the residuals.

```{r LSR_augment_lm}
biomass.lm.augment <- biomass.lm %>%
  broom::augment()
```

This produces a tibble, in which one of the variables, called ".resid", houses the residuals that we're after. 

Let's have a look:

```{r LSR_augment_view}
biomass.lm.augment
```

"Residuals" represent the vertical difference between each observed value of $Y$ (here, biomass stability) the least-squares line (the predicted value of $Y$, ".fitted" in the tibble above) at each value of $X$.  

Don't worry about the other variables in the tibble for now. 

Let's revisit the assumptions:  

* the true relationship between $X$ and $Y$ is linear  
* for every value of $X$ the corresponding values of $Y$ are normally distributed  
* the variance of $Y$-values is the same at all values of $X$
* at each value of $X$, the $Y$ measurements represent a random sample from the population of possible $Y$ values  

With respect to the last assumption regarding random sampling, we can assume that it is met for the plant experiment example.  In experiments, the key experimental design feature is that individuals (here, plant seedlings) are randomly assigned to treatments, and that the individuals themselves were randomly drawn from the population prior to randomizing to experimental treatments. If your study is observational, then it is key that the individuals upon which measurements were made were randomly drawn from the population.

For the first three assumptions, we start by looking at the original scatterplot \@ref(fig:lsrfigone), but this time we add what's called a "locally weighted smoother" line, using the `ggplot2` function `geom_smooth`:

```{r lsrfigtwo, fig.cap = "Stability of biomass production over 10 years in 161 plots and the initial number of plant species assigned to plots. Also shown is a locally weighted smoothing line.", fig.width = 4.2, fig.height = 4} 

plantbiomass %>%
ggplot(aes(x = nSpecies, y = biomassStability)) +
  geom_point(shape = 1) +
  geom_smooth(method = "loess", se = FALSE, formula = y ~ x) +
  xlab("Species richness") +
  ylab("Biomass stability") +
  theme_bw()
```

Note the arguments provided for the `geom_smooth` function.  

The smoothing line will make it easier to assess whether the relationship is linear or not. Sometimes it is obvious from the scatterplot that the association is non-linear (curved) (violating the first assumption), and sometimes it is obvious that the spread of the $Y$ values appears to change systematically (e.g. get bigger) with increasing values of $X$ (violating the third assumption). 

The second assumption is better assessed using the residuals (see below).  

In either case, one can try [transforming](#transform) the response variable, re-running the regression analysis using this transformed variable as the new "_Y'_" variable, then checking the assumptions again. 

#### Outliers {#LSR_outliers}

Outliers will cause the first three regression assumptions to be violated.  

Viewing the scatterplot, keep an eye out for **outliers**, i.e. observations that look very unusual relative to the other observations.  In regression, observations that are far away from the general linear association can have undue influence over the best-fit line.  This sounds a bit fuzzy to judge, and it is.  There are more formal approaches to evaluating "outliers", but for now use this approach: If your eye is drawn to an observation because it seems unusual compared to the rest, then you're probably on to something.  In this case, the most transparent and thorough approach is to report your regression results first using the full dataset, and then second, excluding the outlier observations from the analysis.  

Two other types of graphs are used to help evaluate assumptions. 

We can check the normality of residuals assumption with a normal quantile plot, which you've seen [previously](#quantile_plots).  

For this, we use the "augmented" output we created above.

```{r lsrfigthree, fig.cap = "Normal quantile plot of the residuals from a regression of biomass stability on Species richness for 161 plots", fig.width = 4.2, fig.height=4}
biomass.lm.augment %>%
  ggplot(aes(sample = .resid)) +
  stat_qq(shape = 1, size = 2) +
  stat_qq_line() +
  theme_bw()

```


> Figure \@ref(fig:lsrfigthree) shows that the residuals don't really fall consistently near the line in the normal quantile plot; there is curviture, and increasing values tend to fall further from the line. This suggests the need to **log-transform the data**.  

Before trying a transformation, let's also check the assumptions that (i) the variance of $Y$ is the same at all values of $X$, and (ii) the assumption that the association is linear.  To do this, we plot the residuals against the original "$X$" variable, which here is the number of species initially used in the treatment. Note that we'll add a horizontal line ("segement") at zero for reference:  

```{r lsrfigfour, fig.cap = "Residual plot from a regression of biomass stability on Species richness for 161 plots.", fig.width = 4.2, fig.height = 4}

biomass.lm.augment %>%
  ggplot(aes(x = nSpecies, y = .resid)) +
  geom_point(shape = 1) +
  geom_abline(slope = 0, linetype = "dashed") +
  xlab("Species richness") +
  ylab("Residual") +
  theme_bw()
```

Here we are watching out for:  

* a "funnel" shape, or substantial differences in the spread of the residuals at each value of $X$
* outliers to the general trend  
* a curved pattern  

> Figure \@ref(fig:lsrfigfour) shows that the variance in biomass stability is greatest within the largest species richness treatment. This again suggests the need for a log transformation.

<div class="note">
**REMINDER**: If ever you see a clear outlier when checking assumptions, then a safe approach is to report the regression with and without the outlier point(s) included. If the exclusion of the outlier changes your conclusions, then you should make this clear. 
</div>

### Residual plots when you have missing values

**IF** there are missing values in either the $X$ or $Y$ variables used in the regression, then simply `filter` those NA rows out before plotting the residuals.  

For example, imagine one of the rows in our biomass dataset included a missing value ("NA") in the "nSpecies" variable, then we could use this code for producing the residual plot:

```
biomass.lm.augment %>%
  filter(!is.na(nSpecies)) %>%
  ggplot(aes(x = nSpecies, y = .resid)) +
  geom_point(shape = 1) +
  geom_abline(slope = 0, linetype = "dashed") +
  xlab("Species richness") +
  ylab("Residual") +
  theme_bw()
```

### Transform the data

Let's log-transform the response variable, creating a new variable `log.biomass` using the `mutate` function from the `dplyr` package: 
```{r}
plantbiomass <- plantbiomass %>%
  mutate(log.biomass = log(biomassStability))
```

Now let's re-run the regression analysis:
```{r}
log.biomass.lm <- lm(log.biomass ~ nSpecies, data = plantbiomass)
```

And `augment` the output:
```{r}
log.biomass.lm.augment <- log.biomass.lm %>%
  broom::augment()
```

And re-plot the residual diagnostic plots:  

```{r lsrfigfive, fig.cap = "Normal quantile plot of the residuals from a regression of biomass stability (log transformed) on Species richness for 161 plots", fig.width = 4.2, fig.height=4}

log.biomass.lm.augment %>%
  ggplot(aes(sample = .resid)) +
  stat_qq(shape = 1, size = 2) +
  stat_qq_line() +
  theme_bw()
```

```{r lsrfigsix, fig.cap = "Residual plot from a regression of biomass stability (log transformed) on Species richness for 161 plots.", fig.width = 4.2, fig.height = 4}

log.biomass.lm.augment %>%
  ggplot(aes(x = nSpecies, y = .resid)) +
  geom_point(shape = 1) +
  geom_abline(slope = 0, linetype = "dashed") +
  xlab("Species richness") +
  ylab("Residual") +
  theme_bw()
```

> Figure \@ref(fig:lsrfigfive) shows that the residuals are reasonably normally distributed, and \@ref(fig:lsrfigsix) shows no strong pattern of changing variance in the residuals along $X$, and nor is there an obvious curved pattern to the residuals (which would indicate a non-linear association). We therefore proceed with the analyses using the log-transformed response variable.  

* * *

### Conduct the regression analysis  

We have already conducted the main regression analysis above, using the `lm` function.  We stored the output in the `log.biomass.lm` object. Now, we use the `summary` function to view the entire output from the regression analysis:  

```{r}
summary(log.biomass.lm)
```

What do we need to focus on in this output?  

Under the "Estimate" heading, you see the estimated value of the Intercept (_a_) and of the slope (_b_), which is reported to the right of the predictor variable name ("nSpecies").  

<div class="note">
There are two values of _t_ reported in the table; one associated with the intercept, and one with the slope. These are testing the implied hypotheses that (i) the intercept equals zero and (ii) the slope equals zero. We are typically only interested in the slope.
</div>

At the bottom of the output you'll see a value for the _F_ statistic, just like you saw in the ANOVA [tutorial](#do_anova1). **It is this value of _F_ that you will report in statements about regression (see below).**  

We also see the _P_-value associated with the test of the zero slope null hypothesis, which is identical to the _P_-value reported for the overall regression (at the bottom of the output).  

<div class="flag">
It is important to recognize that an overall significant regression (i.e. slope significantly different from zero) does not necessarily mean that the regression will yield accurate predictions. For this we need to assess the "coefficient of determination", or $R^2$ value (called "multiple R-squared" in the output), which here is `r round(summary(log.biomass.lm)$r.squared, 2)`, and **represents the fraction of the variation in $Y$ that is accounted for by the linear regression model**. A value of this magnitude is indicative of a comparatively weak regression, i.e. one that does has predictive power, but not particularly good predictive power (provided assumptions are met, and we're not extrapolating beyond the range of our $X$ values).
</div>

Before getting to our concluding statement, we need to additionally calculate the 95% confidence interval for the true slope, $\beta$.  

* * *

### Confidence interval for the slope

To calculate the confidence interval for the true slope, $\beta$, we use the `confint` function from the base stats package:  

```
?confint
```

We run this function on the `lm` model output:  

```{r}
confint(log.biomass.lm)
```

This provides the lower and upper limits of the 95% confidence interval for the intercept (top row) and slope (bottom row).  

* * *

### Scatterplot with regression confidence bands

If your regression is significant, then it is recommended to accompany your concluding statement with a scatterplot that includes so-called **confidence bands** around the regression line.  

<div class="flag">
If your regression is not significant, **do not** include a regression line in your scatterplot!  
</div>

We can calculate two types of confidence intervals to show on the scatterplot:  

* the 95% "confidence bands"  
* the 95% "prediction intervals"  

Most of the time we're interested in showing **confidence bands**, which show the 95% confidence limits for the _mean_ value of $Y$ in the population at each value of $X$. In other words, we're more interested in predicting an average value in the population (easier), rather than the value of an individual in the population (much more difficult).  

To display the confidence bands, we use the `geom_smooth` function from the `ggplot2` package:  

```
?geom_smooth
```

Let's give it a try, making sure to spruce it up a bit with some additional options, and including a good figure caption, shown here for your reference (this is the code you'd put after the "r" in the header of the R chunk:  

```
fig.cap = "Stability of biomass production (log transformed) over 10 years in 161 plots and the initial number of plant species assigned to plots. Also shown is the significant least-square regression line (black solid line; see text for details) and the 95% confidence bands (grey shading)."
```

```{r lsrfigseven, fig.cap = "Stability of biomass production (log transformed) over 10 years in 161 plots and the initial number of plant species assigned to plots. Also shown is the significant least-square regression line (black solid line; see text for details) and the 95% confidence bands (grey shading).", fig.width = 4.2, fig.height = 4} 

plantbiomass %>%
ggplot(aes(x = nSpecies, y = log.biomass)) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", colour = "black", 
              se = TRUE, level = 0.95) +
  xlab("Species richness") +
  ylab("Biomass stability (log-transformed)") +
  theme_bw()
```

There are a few key arguments for the `geom_smooth` function:

- the "method" argument tells ggplot to use a least-squares linear model
- the "se = TRUE" argument tells ggplot to add confidence bands
- the "level = 0.95" tells it to use 95% confidence level
- the "colour" argument tells it what colour to make the best-fit regression line

Here we can interpret the figure as follows:  

> Figure \@ref(fig:lsrfigseven) shows that biomass stability (log-transformed) is positively and linearly related to the initial number of plant species assigned to experimental plots, but there is considerable variation that remains unexplained by the least-square regression model.  

* * *

### Concluding statement

Now that we have our new figure including confidence bands (because our regression was significant), and all our regression output (including confidence intervals for the slope), we're ready to provide a concluding statement.  

<div class="note">
We simply report the sample size rather than the degrees of freedom in the parentheses.
</div>

> As seen in Figure \@ref(fig:lsrfigseven), Species richness is a significant predictor of biomass stability: _Biomass stability (log)_ = `r round(coef(log.biomass.lm)[1], 2)` + `r round(coef(log.biomass.lm)[2], 2)`(_Species richness_); _F_ = `r round(summary(log.biomass.lm)$fstatistic[1], 2)`; _n_ = `r length(na.omit(plantbiomass$biomassStability))`; 95% confidence limits for the slope: `r c(round(confint(log.biomass.lm)[2,1], 3), round(confint(log.biomass.lm)[2,2], 3))`; $R^2$ = `r round(summary(log.biomass.lm)$r.squared, 2)`; _P_ < 0.001). Given the relatively low $R^2$ value, predictions from our regression model will not be particularly accurate.   

<div class="note">
It is OK to report the concluding statement and associated regression output using the log-transformed data. However, if you wanted to make predictions using the regression model, it is often desirable to report the back-transformed predicted values (see below).
</div>

* * *

## Making predictions

Even though the $R^2$ value from our regression was rather low, we'll proceed with using the model to make predictions.  

It is not advisable to make predictions beyond the range of X-values upon which the regression was built. So in our case (see Figure \@ref(fig:lsrfigseven)), we would not wish to make predictions using species richness values beyond 16.  Such "extrapolations" are inadvisable.  

To make a prediction using our regression model, use the `predict.lm` function from the base stats package:  

```
?predict.lm
```

If you do not provide new values of `nSpecies` (note that the variable name must be the same as was used when building the model), then it will simply use the old values that were supplied when building the regression model.  

Here, let's create a new tibble called "new.data" that includes new values of `nSpecies` (we'll use 7 and 13 as example values) with which to make predictions:

```{r}
new.data <- tibble(nSpecies = c(7, 13))
```

Now let's make the predictions, and **remember** that these predicted values of biomass stability will be in the log scale:

```{r}
predicted.values <- predict.lm(log.biomass.lm, newdata = new.data)
predicted.values
```

Let's superimpose these predicted values over the original scatterplot, using the `annotate` function from the `ggplot2` package.

We re-use the code from our original `geom_smooth` plot above, and simply add the `annotate` line of code:

```{r lsrfigeight, fig.cap = "Stability of biomass production (log transformed) over 10 years in 161 plots and the initial number of plant species assigned to plots. Also shown are predicted values of stability across values of species richness.", fig.width = 4.2, fig.height = 4} 
plantbiomass %>%
ggplot(aes(x = nSpecies, y = log.biomass)) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", colour = "black", 
              se = TRUE, level = 0.95) +
  annotate("point", x = new.data$nSpecies, y = predicted.values, shape = 2, size = 4) +
  xlab("Species richness") +
  ylab("Biomass stability") +
  theme_bw()
```

As expected, Figure \@ref(fig:lsrfigeight) shows the predicted values exactly where the regression line would be!  We generally don't present such a figure... here we're just doing it for illustration.    

### Back-transforming regression predictions 
 
The regression model we have calculated above is as follows:  

ln(biomass stability) ~ `r round(coef(log.biomass.lm)[1], 2)` + `r round(coef(log.biomass.lm)[2], 2)`(species richness)

Thus, any predictions we make are predictions of ln(y).  It is sometimes desirable to report predicted values back in their original non-transformed state. To do this, follow the instructions in another [tutorial](#back_transform) dealing with data transformations. 

* * *

## Model-I versus Model-II regression

### Definitions

Model-I regression is the type we just learned - ordinary least-squares regression. We fit a regression line that minimizes the squared residuals with respect to the "Y" variable.

Model-II regression or "reduced major axis regression" fits a line that minimizes the squared residuals in both the "Y" and "X" direction.  

### Which one do I use?

The biomass experiment example that we've used in this tutorial involved direct manipulation of species richness (number of species), and the response variable that was measured was biomass stability.  

For two reasons, this experimental study lends itself to Model-I regression.  First, there is good theoretical reasons to expect that manipulation of plant richness can cause changes in plant biomass stability. Thus, there is a causal direction that makes $X$ a natural "independent" variable and $Y$ a "dependent" variable.  Second, the number of plant species was directly controlled in the experiment, and thus had zero uncertainty associated with its values (biomass stability would have had at least some uncertainty in its estimation).  Under either of these conditions, Model-I regression is most appropriate.

Now imagine an observational study in which we randomly sample 40 vegetation plots, and in each, measure plant species richness (number of plant species) and insect species richness.  We are interested in predicting insect richness from plant richness, but is Model-I regression appropriate?

This study arguably lends itself more to Model-II regression.  Why?  First, because it is conceivable that either plant species richness OR insect species richness could be a "dependent" variable or "independent" variable... the causal direction is not clear.  Second, both variables were not directly controlled, and therefore there is necessarily some uncertainty involved.  

There is some debate about which method should be used in what contexts. If you're interested in learning more, see this [publication](https://doi.org/10.1017/S1464793106007007) and this [one](https://doi.org/10.1002/ajpa.21090). 

For this course, you can assume Model-I regression is appropriate for quiz/exam questions, unless otherwise indicated.  



